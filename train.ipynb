{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer, TweetTokenizer, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import classify, NaiveBayesClassifier\n",
    "import contractions, re, string, random\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the trainung dataset and store in a dataframe\n",
    "train_df = pd.read_csv(\"Training_data.csv\").dropna()\n",
    "train_df2 = pd.read_csv(\"IMDB Dataset.csv\").dropna()\n",
    "train_df = pd.concat([train_df, train_df2])\n",
    "test_df = pd.read_csv(\"Testing_data.csv\").dropna()\n",
    "train_data_text = [str(val) for val in train_df[\"Review\"].values]\n",
    "test_data_text = [str(val) for val in test_df[\"Review\"].values]\n",
    "train_data_sentiment = [str(val) for val in train_df[\"Sentiment\"].values]\n",
    "test_data_sentiment = [str(val) for val in test_df[\"Sentiment\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_tweets\n",
    "def clean(text: str):\n",
    "    # remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # expand contraction\n",
    "    text = contractions.fix(text)\n",
    "    #remove the retweet sign\n",
    "    text = re.sub(r\"^(RT)\", \"\", text)\n",
    "    # remove links\n",
    "    text = re.sub(r\"(ftp|http[s]?)://\\S+\", \"\", text)\n",
    "    # remove mentions\n",
    "    text = re.sub(r\"@[A-Za-z0-9_]+\", \"\", text)\n",
    "    # remove hashtags\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    # remove punctuations\n",
    "    sub_txt = r\"[\" + re.escape(string.punctuation) + r\"]\"\n",
    "    text = re.sub(sub_txt, \"\", text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_text = [clean(tweet) for tweet in train_data_text]\n",
    "test_data_text = [clean(tweet) for tweet in test_data_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "def lemmatize_sentence(token: list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = []\n",
    "    for word, tag in pos_tag(token):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            tag = \"n\" # noun\n",
    "        elif tag.startswith(\"V\"):\n",
    "            tag = \"v\" # verb\n",
    "        elif tag.startswith((\"RB\")):\n",
    "            tag = \"r\" # adverb\n",
    "        else:\n",
    "            tag = \"a\" # adjective\n",
    "        sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return sentence\n",
    "\n",
    "# split texts into tokens\n",
    "tokenized_train_text = [tokenizer.tokenize(text) for text in train_data_text]\n",
    "lemmatized_train_text = [lemmatize_sentence(token) for token in tokenized_train_text]\n",
    "# lemmatized_data_text\n",
    "tokenized_test_text = [tokenizer.tokenize(text) for text in test_data_text]\n",
    "lemmatized_test_text = [lemmatize_sentence(token) for token in tokenized_test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def remove_stopwords(token: list):\n",
    "    cleaned_up = []\n",
    "    for word in token:\n",
    "        if len(word) > 1 and word not in string.punctuation and word.lower() not in stop_words:\n",
    "            cleaned_up.append(word.lower())\n",
    "    return cleaned_up\n",
    "    \n",
    "cleaned_train_text = [remove_stopwords(text) for text in lemmatized_train_text]\n",
    "cleaned_test_text = [remove_stopwords(text) for text in lemmatized_test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare text for model\n",
    "# the model requires a dictionary of word with with values as True\n",
    "def get_dict_for_model(cleaned_tokenized_text: list):\n",
    "    for tokens in cleaned_tokenized_text:\n",
    "        yield dict((word, True) for word in tokens)\n",
    "\n",
    "train_data = [(text_dict, train_data_sentiment[n]) for n, text_dict in enumerate(get_dict_for_model(cleaned_train_text))]\n",
    "test_data = [(text_dict, test_data_sentiment[n]) for n, text_dict in enumerate(get_dict_for_model(cleaned_test_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91736\n",
      "Most Informative Features\n",
      "                     uwe = True           negati : positi =     59.8 : 1.0\n",
      "                    boll = True           negati : positi =     41.6 : 1.0\n",
      "                    tsui = True           positi : negati =     32.3 : 1.0\n",
      "            interminable = True           negati : positi =     30.2 : 1.0\n",
      "                 antwone = True           positi : negati =     29.7 : 1.0\n",
      "                    icet = True           negati : positi =     29.7 : 1.0\n",
      "               deathtrap = True           positi : negati =     27.0 : 1.0\n",
      "                   ronny = True           positi : negati =     27.0 : 1.0\n",
      "                   hayao = True           positi : negati =     25.7 : 1.0\n",
      "               toughness = True           positi : negati =     25.7 : 1.0\n",
      "['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "analyzer = NaiveBayesClassifier.train(train_data)\n",
    "print(\"Accuracy: \" + str(classify.accuracy(analyzer, test_data)))\n",
    "with open(\"NBModel\", 'wb') as f:\n",
    "    pickle.dump(analyzer, f)\n",
    "analyzer.show_most_informative_features(10)\n",
    "print(analyzer.labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(text: str):\n",
    "    text = clean(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    lemmatized_text = lemmatize_sentence(tokenized_text)\n",
    "    cleaned_text = remove_stopwords(lemmatized_text)\n",
    "    return analyzer.classify(dict([(token, True) for token in cleaned_text]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
